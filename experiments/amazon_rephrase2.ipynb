{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Calculate scores of sensitivity and informativeness for the Amazon reviews"
   ],
   "metadata": {
    "id": "PhPDrpEEK4R1"
   },
   "id": "PhPDrpEEK4R1"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q typing-extensions openai tiktoken"
   ],
   "metadata": {
    "id": "OuJXCfDtxy-Q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1f379073-18dc-44af-8845-76513288ebd3",
    "is_executing": true
   },
   "id": "OuJXCfDtxy-Q",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-03T04:18:30.837685Z",
     "start_time": "2024-02-03T04:18:30.835755Z"
    },
    "id": "initial_id",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6a7c5d53-3d34-4c7c-8074-7b08ff2eec41"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import json\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import openai\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import statistics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Glossary and Guidance to Use:\n",
    "1. If need to start over the experiment again, update the experiment_no. Otherwise it will resume the progress from last time\n",
    "2. If need to modify prompt template, do update prompt_id, prompt_template, COMMENT(as detailed as possible). (AND probably you also should update experiment_no)\n",
    "3. If want to experiment on more reviews, just modify used_reviews and prompt_repeat_times, and then run the whole notebook again, but beware previously processed reviews won't be affected.\n",
    "4. The output file is a large large .csv file. Functionalities like hiding, filtering, ordering, functions in Google Excel or MS Excel will help you to do the analysis you want. The large amount of information we record is meant for potential future tracking.\n",
    "'''\n",
    "\n",
    "# ==================\n",
    "# Configurable Parameters\n",
    "\n",
    "# the number of experiment, increment this means restarting over the experiment, it will reset the checkpoint and the working progress\n",
    "experiment_no = 20\n",
    "\n",
    "# Prompt Definitions. Always use {reviews} as a placeholder for review text. Need to edit code before adding any other new placehodlers. Every other place that needs a bracket, please use double brackets instead.\n",
    "prompt_id = 1\n",
    "prompt_template = '''The following is a review of an individual's experience with a health product on Amazon. Please start by identifying any health-sensitive information about the author included in this review.\n",
    "Propose a sanitized version of the text, in the voice of the reviewer, that maintains its informativeness while removing sensitive details.\n",
    "\n",
    "\n",
    "Health-sensitive information includes, but is not limited to, details regarding medical conditions, treatments, or personal health experiences.\n",
    "Your output should have the following JSON format:\n",
    "{{\n",
    "\"sens_info\": Identify each piece of sensitive information included in the review. This field should be left blank if the text is insensitive.\n",
    "\"rephrased\": Propose a way to sanitize the original text from sensitive information while leaving unrelated words unchanged. This field should be left blank if the text is insensitive.\n",
    "\"explanation\": Explain why you rephrased different parts of the review and how this makes it less sensitive. This field should be left blank if the text is insensitive.\n",
    "\"sens_score\": On a scale of 0 to 10, where 0 is no sensitive information, and 10 is a lot of sensitive information, rate the sensitivity of the original review.\n",
    "\"sens_reph\": On the same scale, rate the sensitivity of the original review. This field should be left blank if the text is insensitive.\n",
    "\"info_score\": On a scale of 0 to 10, where 0 is useless to people who read the review, and 10 is highly informative to people who read the review, rate the original review.\n",
    "\"info_reph\": On the same scale, rate the informativeness of the rephrased review. This field should be left blank if the text is insensitive.\n",
    "}}\n",
    "\n",
    "{reviews}\n",
    "\n",
    "'''\n",
    "\n",
    "old_prompt = '''I will give you a product review from Amazon and you will have to tell me if that string contains potentially medically sensitive information and help me rephrase it if so.\n",
    "\n",
    "Your output should have the following JSON format:\n",
    "{{\n",
    "\"sens_info\": Identify each piece of sensitive information included in the review. This field should be left blank if the text is insensitive.\n",
    "\"rephrased\": Propose a way to sanitize the original text from sensitive information while leaving unrelated words unchanged. This field should be left blank if the text is insensitive.\n",
    "\"explanation\": Explain why you rephrased different parts of the review and how this makes it less sensitive. This field should be left blank if the text is insensitive.\n",
    "\"sens_score\": On a scale of 0 to 10, where 0 is no sensitive information, and 10 is a lot of sensitive information, rate the sensitivity of the original review.\n",
    "\"sens_reph\": On the same scale, rate the sensitivity of the original review. This field should be left blank if the text is insensitive.\n",
    "\"info_score\": On a scale of 0 to 10, where 0 is useless to people who read the review, and 10 is highly informative to people who read the review, rate the original review.\n",
    "\"info_reph\": On the same scale, rate the informativeness of the rephrased review. This field should be left blank if the text is insensitive.\n",
    "}}\n",
    "\n",
    "\n",
    "{reviews}\n",
    "\n",
    "'''\n",
    "\n",
    "# comment before every experiment, need to be filled for future looking up.\n",
    "comment = ''' Old prompt with 10 tries\n",
    "\n",
    "'''\n",
    "# the prompt to use\n",
    "used_prompt_template = old_prompt\n",
    "# how many times to repeat the prompt\n",
    "prompt_repeat_times = 10\n",
    "# how many reviews to use\n",
    "used_reviews = 28\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T04:18:30.915309Z",
     "start_time": "2024-02-03T04:18:30.911465Z"
    },
    "id": "e976848931f70301"
   },
   "id": "e976848931f70301"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for openai api key and fail if not found\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY environment variable not found, set it manually here or the script will fail.\")\n",
    "    openai.api_key = \"<OpenAI_key_goes_here>\"\n",
    "\n",
    "model = \"gpt-4\"\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# ==================\n",
    "# cols of the output file\n",
    "basic_cols = ['experiment_id', 'comment', 'prompt_id', 'prompt_template', 'input_file', 'review_id', 'review_text',\n",
    "              'trial_no']\n",
    "statistical_cols = ['sens_orig_avg', 'sens_reph_avg', 'info_orig_avg', 'info_reph_avg', 'sens_orig_var',\n",
    "                    'sens_reph_var', 'info_orig_var', 'info_reph_var']\n",
    "detail_cols = [\"sens_orig\", \"sens_reph\", \"info_orig\", \"info_reph\", \"rephrased_text\", \"failed\", \"explanation\", \"output\",\n",
    "               \"price\"]\n",
    "\n",
    "# required file paths\n",
    "\n",
    "input_path = r'../data/all_reviews_28.csv'\n",
    "output_path = r'../output'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "checkpoint_path = r'../checkpoint'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_file = f'{checkpoint_path}/ckpt{experiment_no}'\n",
    "if not os.path.exists(checkpoint_file):\n",
    "    os.makedirs(checkpoint_file)\n",
    "\n",
    "# default value for missing scores in model's answers\n",
    "default_score_for_missing = -1\n",
    "\n",
    "input_pandas = pd.read_csv(input_path)[0:used_reviews]"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T04:18:30.911127Z",
     "start_time": "2024-02-03T04:18:30.840919Z"
    },
    "id": "388e218a57265d79"
   },
   "id": "388e218a57265d79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Function Definitions\n",
    "def num_tokens(model, string) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "\n",
    "def access_gpt(prompt):\n",
    "    \"\"\"return the output and price of a specific prompt\"\"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    output = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            output += chunk.choices[0].delta.content\n",
    "\n",
    "    price = num_tokens(model, prompt) * 0.00003 + num_tokens(model, prompt) * 0.00006\n",
    "    return [output, price]\n",
    "\n",
    "\n",
    "def process_review_multi_threaded(review, prompt_repeat_times, used_prompt_template):\n",
    "    gpt_results = []\n",
    "    prompt = used_prompt_template.format(reviews=review)\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        # Submit tasks to the executor\n",
    "        future_to_gpt = {executor.submit(access_gpt, prompt): _ for _ in range(prompt_repeat_times)}\n",
    "        for future in concurrent.futures.as_completed(future_to_gpt):\n",
    "            try:\n",
    "                gpt_results.append(future.result())  # Append the result of access_gpt\n",
    "            except Exception as exc:\n",
    "                print(f'Generated an exception: {exc}')\n",
    "    return gpt_results\n",
    "\n",
    "\n",
    "def parse_raw_answers(raw_answers):\n",
    "    \"\"\"return the sensitivity and informativeness scores of the original and rephrased reviews. works for batch\"\"\"\n",
    "    parsed_results = []\n",
    "\n",
    "    for gpt_answer in raw_answers:\n",
    "        price_result = gpt_answer[1]\n",
    "        parsed_result = []\n",
    "        # looks like this:\n",
    "        # {\\n\"sens_info\": \"\",\\n\"rephrased\": \"\",\\n\"explanation\": \"\",\\n\"sens_score\": 0,\\n\"sens_reph\": 0,\\n\"info_score\": 6,\\n\"info_reph\": 6\\n}\n",
    "        sens_score_rephrased = -1\n",
    "        sens_score_original = -1\n",
    "        info_score_original = -1\n",
    "        info_score_rephrased = -1\n",
    "        explanation = 'FAIL'\n",
    "        rephrased_text = 'FAIL'\n",
    "        price = 0\n",
    "        raw_answer = gpt_answer\n",
    "        try:\n",
    "            review_data = json.loads(gpt_answer[0])\n",
    "\n",
    "            def get_score(value):\n",
    "                return value if isinstance(value, (int, float)) else default_score_for_missing\n",
    "\n",
    "            sens_score_rephrased = get_score(review_data.get('sens_reph', default_score_for_missing))\n",
    "            sens_score_original = get_score(review_data.get('sens_score', default_score_for_missing))\n",
    "            info_score_original = get_score(review_data.get('info_score', default_score_for_missing))\n",
    "            info_score_rephrased = get_score(review_data.get('info_reph', default_score_for_missing))\n",
    "            explanation = review_data.get('explanation', \"\")\n",
    "            rephrased_text = review_data.get('rephrased', \"\")\n",
    "            price = price_result\n",
    "            fail_to_parse = False\n",
    "        except json.JSONDecodeError:\n",
    "            fail_to_parse = True\n",
    "\n",
    "        # append\n",
    "        parsed_result.append(sens_score_original)\n",
    "        parsed_result.append(sens_score_rephrased)\n",
    "        parsed_result.append(info_score_original)\n",
    "        parsed_result.append(info_score_rephrased)\n",
    "        parsed_result.append(rephrased_text)\n",
    "        parsed_result.append(fail_to_parse)\n",
    "        parsed_result.append(explanation)\n",
    "        parsed_result.append(raw_answer)\n",
    "        parsed_result.append(price)\n",
    "\n",
    "        # add to results\n",
    "        parsed_results.append(parsed_result)\n",
    "\n",
    "    return parsed_results\n",
    "\n",
    "\n",
    "def get_basic_cols():\n",
    "    return basic_cols\n",
    "\n",
    "\n",
    "def fill_in_basic_cols(review_id, review_text, trial_no):\n",
    "    return [experiment_no, comment, prompt_id, used_prompt_template, input_path, review_id, review_text, trial_no]\n",
    "\n",
    "\n",
    "def get_statistical_cols():\n",
    "    return statistical_cols\n",
    "\n",
    "\n",
    "def fill_in_statistical_cols(details):\n",
    "    filtered_details = [detail for detail in details if default_score_for_missing not in detail[:4]]\n",
    "\n",
    "    def all_identical(lst):\n",
    "        return [lst[0]] * len(lst) == lst\n",
    "\n",
    "    return [statistics.mean(detail[0] for detail in filtered_details) if len(filtered_details) > 0 else -1,\n",
    "            #'sens_orig_avg'\n",
    "            statistics.mean(detail[1] for detail in filtered_details) if len(filtered_details) > 0 else -1,\n",
    "            #'sens_reph_avg',\n",
    "            statistics.mean(detail[2] for detail in filtered_details) if len(filtered_details) > 0 else -1,\n",
    "            #'info_orig_avg',\n",
    "            statistics.mean(detail[3] for detail in filtered_details) if len(filtered_details) > 0 else -1,\n",
    "            #'info_reph_avg',\n",
    "            statistics.variance(detail[0] for detail in filtered_details) if len(\n",
    "                filtered_details) > 1 and not all_identical([detail[0] for detail in filtered_details]) else 0,\n",
    "            #'sens_orig_var',\n",
    "            statistics.variance(detail[1] for detail in filtered_details) if len(\n",
    "                filtered_details) > 1 and not all_identical([detail[1] for detail in filtered_details]) else 0,\n",
    "            #'sens_reph_var',\n",
    "            statistics.variance(detail[2] for detail in filtered_details) if len(\n",
    "                filtered_details) > 1 and not all_identical([detail[2] for detail in filtered_details]) else 0,\n",
    "            #'info_orig_var',\n",
    "            statistics.variance(detail[3] for detail in filtered_details) if len(\n",
    "                filtered_details) > 1 and not all_identical([detail[3] for detail in filtered_details]) else 0\n",
    "            #'info_reph_var'\n",
    "            ]\n",
    "\n",
    "\n",
    "def get_detailed_cols(repeat=3):\n",
    "    return detail_cols\n",
    "\n",
    "\n",
    "def fill_in_detail_cols(answer_parsed):\n",
    "    return answer_parsed"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T04:18:30.922045Z",
     "start_time": "2024-02-03T04:18:30.918157Z"
    },
    "id": "4aff15ddd504458c"
   },
   "id": "4aff15ddd504458c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import os\n",
    "\n",
    "now = dt.now()\n",
    "output_file_prefix = f'{now.strftime(\"%Y-%m-%d_%H:%M:%S\")}-{model}-exp{experiment_no}'\n",
    "\n",
    "review_no = 0\n",
    "file_name = f'{output_file_prefix}.csv'\n",
    "\n",
    "# ==================\n",
    "# open checkpoint if already present and load the state of last run, if so required\n",
    "if os.path.isfile(checkpoint_file):\n",
    "    ckpt = pd.read_csv(checkpoint_file)\n",
    "    try:\n",
    "        review_no = int(ckpt['review_no'].tail(1)) + 1\n",
    "        file_name = ckpt.tail(1)['csv_name'].values.tolist()[-1]\n",
    "    except:\n",
    "        print(\"FAIL TO PARSE CHECKPOINT!\")\n",
    "        pass\n",
    "else:\n",
    "    with open(f'{checkpoint_file}', 'w+') as ckpf:\n",
    "        w = csv.writer(ckpf)\n",
    "        w.writerow([\"csv_name\", \"review_no\"])\n",
    "\n",
    "# ==================\n",
    "# access gpt -> parse the answer -> Create Aggregated DataFrame -> write output to file\n",
    "\n",
    "with (open(f'{output_path}/{file_name}', 'a+') as f, open(checkpoint_file, 'a') as ckpf):\n",
    "    f_writer = csv.writer(f)\n",
    "    ckpw = csv.writer(ckpf)\n",
    "    headers = get_basic_cols() + get_statistical_cols() + get_detailed_cols()\n",
    "    if review_no == 0:\n",
    "        f_writer.writerow(headers)\n",
    "\n",
    "    review_list = input_pandas['reviews'].values.tolist()[review_no:]\n",
    "    for num, review_text in enumerate(review_list, review_no):\n",
    "        raw_model_answers = process_review_multi_threaded(review_text, prompt_repeat_times, used_prompt_template)\n",
    "\n",
    "        answers_parsed = parse_raw_answers(raw_model_answers)\n",
    "\n",
    "        for trial_no in range(prompt_repeat_times):\n",
    "            basic_cols_fill = fill_in_basic_cols(num, review_text, trial_no)\n",
    "            statistical_cols_fill = fill_in_statistical_cols(answers_parsed)\n",
    "            detail_cols_fill = fill_in_detail_cols(answers_parsed[trial_no])\n",
    "            f_writer.writerow([*basic_cols_fill, *statistical_cols_fill, *detail_cols_fill])\n",
    "        # save to checkpoint\n",
    "        ckpw.writerow([file_name, num])\n",
    "        print(f'review {num} of {used_reviews} is finished for experiment {experiment_no}')\n"
   ],
   "metadata": {
    "id": "e6268558c31f3cc7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "20761a41-2150-4564-b7ab-d3be9b112c6a"
   },
   "id": "e6268558c31f3cc7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
